## 深度模型训练



### README.md

然后直接train模型，单GPU运行，模型自选：

```
python train_predict.py --gpu 4 --option 5 --model convlstm --feature char
```

多GPU训练示例：

```
python train_predict.py --gpu 4,5,6,7 --option 5 --model convlstm --feature char
```

### 

### config.py

这个文件定义了一个Config类用于读取深度模型训练时所需的各种参数

```python
from model.lightgbm_model import LightGbmModel
# from model.xgboost_model import XgboostModel
from model.textcnn_model import TextCNNModel
from model.lstmgru_model import LstmgruModel
from model.attention import AttentionModel
from model.attention1 import AttentionNoLamdaModel
from model.convlstm_model import ConvlstmModel
from model.dpcnn_model import DpcnnModel
from model.capsule_model import CapsuleModel
from model.rcnn_model import RCNNModel
from model.textrnn_model import TextRNNModel
from model.attrnn_model import AttentionRNNModel
from model.simplecnn_model import SimpleCNNModel

class Config(object):

    """Docstring for Config. """

    def __init__(self):
        """TODO: to be defined1. """
        self.model = {
            # 'xgboost': XgboostModel,
            'lightgbm': LightGbmModel,

            # dl model
            'textcnn': TextCNNModel,
            'textrnn': TextRNNModel,
            'lstmgru': LstmgruModel,
            'attention': AttentionModel,
            'attention1': AttentionNoLamdaModel,
            'rnn_attention': AttentionRNNModel,
            'convlstm': ConvlstmModel,
            'dpcnn': DpcnnModel,
            'rcnn': RCNNModel,
            'capsule': CapsuleModel,
            'simplecnn': SimpleCNNModel,
        }
        self.n_class = 19
        # 9-5
        # self.WORD_MAXLEN = 1000
        self.CHAR_MAXLEN = 2000
        self.WORD_MAXLEN = self.CHAR_MAXLEN

        self.EMBED_SIZE = 100
        self.BATCH_SIZE = 128
        self.main_feature = 'word'
        self.wd = 1e-6
        self.date = '0908'

        #  self.article_w2v_file = '../data/word2vec-models/word2vec.char.{}d.model'.format(self.EMBED_SIZE)
        #  self.word_seg_w2v_file = '../data/word2vec-models/word2vec.word.{}d.model'.format(self.EMBED_SIZE)
        self.article_w2v_file = '../data/word2vec-models/word2vec.char.{}d.mfreq3.model'.format(self.EMBED_SIZE)
        self.word_seg_w2v_file = '../data/word2vec-models/word2vec.word.{}d.mfreq3.model'.format(self.EMBED_SIZE)
        self.chi_word_file = '../data/chi_words_40000.pkl'
        self.chi_char_file = '../data/chi_char_3000.pkl'

        # self.TRAIN_X = '../data/Clean.train.csv'
        self.TRAIN_X = '../data/New.train.csv'
        # self.TRAIN_X = '../data/Enhance.train.csv'
        self.TEST_X = '../data/New.test.csv'


```



### train_predict.py

#### 第一步：import argparse

```python
import argparse
parser = argparse.ArgumentParser()
parser.add_argument('--gpu', type=str, default='6')
parser.add_argument('--option', type=int)
parser.add_argument('--model', type=str)
parser.add_argument('--feature', default='word', type=str)
args = parser.parse_args()
os.environ["CUDA_VISIBLE_DEVICES"] = args.gpu
```

这部分的作用是实现

```
python train_predict.py --gpu 4 --option 5 --model convlstm --feature char
```

的语法的编译，默认的gpu device_id = 6，然后通过这个语句定义了几个参数

将所有参数都收录在args中

然后将CUDA_VISIBLE_DEVICES设定为gpu的 device_id参数，此处为4



#### 第二步：deep_data_process

```python
import tensorflow as tf
from keras.backend.tensorflow_backend import set_session
tf_config = tf.ConfigProto()
# tf_config.gpu_options.per_process_gpu_memory_fraction = 0.8
set_session(tf.Session(config=tf_config))

config = Config()
config.option = args.option
config.n_gpus = len(args.gpu.split(','))
config.main_feature = args.feature
config.model_name = args.model
# static_mode_process()
deep_data_process()
```

这一部分是深度学习的部分，前面都是定义，核心函数是deep_data_process()



##### deep_data_process()

```python
def deep_data_process():
    # deep_data_cache()
    train, train_y, test, char_init_embed, word_init_embed = pickle.load(open('../data/cache/clean_deep_data_{}_{}.pkl'.format(config.WORD_MAXLEN, config.EMBED_SIZE), 'rb'))
    config.max_c_features = len(char_init_embed)
    config.max_w_features = len(word_init_embed)
    config.char_init_embed = char_init_embed
    config.word_init_embed = word_init_embed

    model = config.model[args.model](config=config, num_flods=5)
    model.train_predict(train, train_y, test, option=config.option)
    # model.rerun(test_x)
```

在这个函数中，读取了一个暂时没有的clean_deep_data_{}_{}.pkl，这个文件应该是由之前备注未运行的[deep_data_cache](#deep_data_cache())产生的。

这个文件包含训练所用的训练X，y，test和对应的embed_matrix

将embed_matrix定义到config里。args.model 在句中为 convlstm，在config内对应ConvlstmModel，在[config.py](#config.py)内是从model.convlstm_model中import的。对应的文件在model文件夹下的[convlstm_model.py](#convlstm_model.py)

然后用train_predict函数进行模型训练。



##### deep_data_cache()

```python
def deep_data_cache():
    article_word2vec_model = Word2Vec.load(config.article_w2v_file)
    word_seg_word2vec_model = Word2Vec.load(config.word_seg_w2v_file)
    train, train_y, test = deep_data_prepare(config, article_word2vec_model, word_seg_word2vec_model)
    char_init_embed = init_embedding(config, article_word2vec_model)
    word_init_embed = init_embedding(config, word_seg_word2vec_model)
    os.makedirs('../data/cache/', exist_ok=True)
    pickle.dump((train,  train_y, test, char_init_embed, word_init_embed), open('../data/cache/clean_deep_data_{}_{}.pkl'.format(config.WORD_MAXLEN, config.EMBED_SIZE), 'wb'))
```



> 从这里就可以看到定义一个Config.py的好处，在修改EMBED_SIZE的时候，只需要在Config.py修改这个值就可以了，生成的数据文件和模型文件名都带有这个参数的量，可以实现自动选择读取的文件。

deep_data_cache()先读取了之前EDA时word2vec所生成的word2vec模型文件，然后读取了[deep_data_prepare](#deep_data_prepare())。返回了深度学习所需要的数据train，train_y，test。

之后用函数[init_embedding](#init_embedding())做了一个初始embedding。将embedding之后的数据保存为clean_deep_data保存在cache文件夹下。



##### deep_data_prepare()

```python
def deep_data_prepare(config, article_word2vec_model, word_seg_word2vec_model):
    print('深度学习模型数据准备')
    print('加载训练数据与测试数据')
    train_df = pd.read_csv(config.TRAIN_X)
    test_df = pd.read_csv(config.TEST_X)

    char_sw_list = pickle.load(open('../data/char_stopword.pkl', 'rb'))
    word_sw_list = pickle.load(open('../data/word_stopword.pkl', 'rb'))
    chi_w_list = pickle.load(open(config.chi_word_file, 'rb'))
    chi_c_list = pickle.load(open(config.chi_char_file, 'rb'))
    # 用词向量
    # 用字向量
    train_char = train_df['article']
    train_word = train_df['word_seg']
    test_char = test_df['article']
    test_word = test_df['word_seg']

    train_y = train_df['c_numerical'].values
    UNK_CHAR = len(article_word2vec_model.stoi) + 1
    PAD_CHAR = len(article_word2vec_model.stoi)

    UNK_WORD = len(word_seg_word2vec_model.stoi) + 1
    PAD_WORD = len(word_seg_word2vec_model.stoi)

    def word2id(train_dialogs, type='char', filter=True):
        if type == 'char':
            word2vec_model = article_word2vec_model
            max_len = config.CHAR_MAXLEN
            UNK = UNK_CHAR
            sw_list = set(char_sw_list)
            chi_lst = set(chi_c_list)
        elif type == 'word':
            word2vec_model = word_seg_word2vec_model
            max_len = config.WORD_MAXLEN
            UNK = UNK_WORD
            sw_list = set(word_sw_list)
            chi_lst = set(chi_w_list)
        else:
            exit('类型错误')

        print('卡方词表长：', len(chi_lst))
        train_x = []
        for d in tqdm(train_dialogs):
            d = d.split()
            line = []
            for token in d:
                if token in sw_list\
                        or token == ''\
                        or token == ' ':
                        # or token not in chi_lst\
                    continue
                if token in word2vec_model.stoi:
                    line.append(word2vec_model.stoi[token])
                else:
                    line.append(UNK)

            train_x.append(line[:max_len])
            # if type == 'word':
                # train_x.append(line[:max_len])
            # else:
                # train_x.append(line[-max_len:])
        return train_x

    train_x_word = word2id(train_word, type='word')
    train_x_char = word2id(train_char, type='char')
    test_x_char = word2id(test_char, type='char')
    test_x_word = word2id(test_word, type='word')

    train_word_left = [[UNK_WORD] + w[:-1] for w in train_x_word]
    train_word_right = [w[1:] + [UNK_WORD] for w in train_x_word]
    train_char_left = [[UNK_CHAR] + w[:-1] for w in train_x_char]
    train_char_right = [w[1:] + [UNK_CHAR] for w in train_x_char]

    test_word_left = [[UNK_WORD] + w[:-1] for w in test_x_word]
    test_word_right = [w[1:] + [UNK_WORD] for w in test_x_word]
    test_char_left = [[UNK_CHAR] + w[:-1] for w in test_x_char]
    test_char_right = [w[1:] + [UNK_CHAR] for w in test_x_char]

    UNK_CHAR = PAD_CHAR
    UNK_WORD = PAD_WORD
    train_x_char = sequence.pad_sequences(train_x_char, maxlen=config.CHAR_MAXLEN, dtype='int32', padding='post', truncating='post', value=UNK_CHAR)
    test_x_char = sequence.pad_sequences(test_x_char, maxlen=config.CHAR_MAXLEN, dtype='int32', padding='post', truncating='post', value=UNK_CHAR)
    train_x_word = sequence.pad_sequences(train_x_word, maxlen=config.WORD_MAXLEN, dtype='int32', padding='post', truncating='post', value=UNK_WORD)
    test_x_word = sequence.pad_sequences(test_x_word, maxlen=config.WORD_MAXLEN, dtype='int32', padding='post', truncating='post', value=UNK_WORD)

    train_x_char_left = sequence.pad_sequences(train_char_left, maxlen=config.CHAR_MAXLEN, dtype='int32', padding='post', truncating='post', value=UNK_CHAR)
    test_x_char_left = sequence.pad_sequences(test_char_left, maxlen=config.CHAR_MAXLEN, dtype='int32', padding='post', truncating='post', value=UNK_CHAR)
    train_x_word_left = sequence.pad_sequences(train_word_left, maxlen=config.WORD_MAXLEN, dtype='int32', padding='post', truncating='post', value=UNK_WORD)
    test_x_word_left = sequence.pad_sequences(test_word_left, maxlen=config.WORD_MAXLEN, dtype='int32', padding='post', truncating='post', value=UNK_WORD)

    train_x_char_right = sequence.pad_sequences(train_char_right, maxlen=config.CHAR_MAXLEN, dtype='int32', padding='post', truncating='post', value=UNK_CHAR)
    test_x_char_right = sequence.pad_sequences(test_char_right, maxlen=config.CHAR_MAXLEN, dtype='int32', padding='post', truncating='post', value=UNK_CHAR)
    train_x_word_right = sequence.pad_sequences(train_word_right, maxlen=config.WORD_MAXLEN, dtype='int32', padding='post', truncating='post', value=UNK_WORD)
    test_x_word_right = sequence.pad_sequences(test_word_right, maxlen=config.WORD_MAXLEN, dtype='int32', padding='post', truncating='post', value=UNK_WORD)

    train_y = np_utils.to_categorical(train_y, num_classes=config.n_class)
    print('train_x char shape is: ', train_x_char.shape)
    print('test_x char shape is: ', test_x_char.shape)
    print('train_x word shape is: ', train_x_word.shape)
    print('test_x word shape is: ', test_x_word.shape)

    print('train_y shape is: ', train_y.shape)
    train = {}
    train['word'] = train_x_word
    train['char'] = train_x_char
    train['word_left'] = train_x_word_left
    train['word_right'] = train_x_word_right
    train['char_left'] = train_x_char_left
    train['char_right'] = train_x_char_right
    test = {}
    test['word'] = test_x_word
    test['char'] = test_x_char
    test['word_left'] = test_x_word_left
    test['word_right'] = test_x_word_right
    test['char_left'] = test_x_char_left
    test['char_right'] = test_x_char_right
    assert train['word_left'].shape == train['word_right'].shape == train['word'].shape
    assert train['char_left'].shape == train['char_right'].shape == train['char'].shape
    assert test['word_left'].shape == test['word_right'].shape == test['word'].shape
    assert test['char_left'].shape == test['char_right'].shape == test['char'].shape
    return train, train_y, test

```

deep_data_prepare做了以下八件事：

1. 读取了经过低频词处理和数据增强处理后的数据

2. 用char_sw_list，word_sw_list读取了之前提取的低频词列表长度各3361，529805

3. 用chi_w_list，chi_c_list读取了之前通过卡方方法提取的卡方词表长度各822861,13516

   > 注意这里读取了卡方词表，那么就需要在word2id函数里将 chi_lst 纳入循环条件，可以排除掉一部分不在卡方词表里的词

4. UNK_CHAR，PAD_CHAR，UNK_WORD，PAD_WORD为word2vec模型的stoi

   > stoi 即词频表{‘7368’ ： 1， ‘1252059’ ： 372， ...}
   >
   > PAD_CHAR = 10528，UNK_CHAR = 10529，PAD_WORD = 359280，UNK_WORD = 359279

5. 用函数word2id对train_df的article列和word_seg列分别做了处理

   - train_x只收录了非""," "的在卡方词表内的非低频词

   - 得到的train_x_word为

     ```
     [[1,  2056,  0,  8018,  2,  2007,  501,  21,  0,  162,  3847,  355,  13,  18,  15796,  3,  5635,  2748,......]]
     ```

6. 在得到train_x_word后，对train_x_word做了左右平移的处理，处理后得到的train_word_left为

   ```
   [[359279,  1,  2056,  0,  8018,  2,  2007,  501,  21,  0,  162,
     3847,  355,  13,  18,  15796,  3,  5635,  2748,  636,......]]
   ```

   可见左移的结果就是移除了最右边的元素然后在train_x_word的最左边加入了UNK_WORD = 359279。

   对应右移就是移除了最左边的元素然后在右边加入了UNK_WORD。

7. 用sequence.pad_sequences对得到的train_x_word，进行了处理，得到了train_x_char

   由于config的CHAR_MAXLEN设定为2000，所以train_x_char的shape为(102277, 2000)。

   ```python
   In: train_x_char[:5]
   Out: array([[    1,   372,    33, ..., 10528, 10528, 10528],
          [   12,    98,     1, ...,    87,     9,     4],
          [    1,   352,   676, ...,  1321,   207,   552],
          [  140,    22,   140, ..., 10528, 10528, 10528],
          [    1,     1,     1, ..., 10528, 10528, 10528]])
   ```

8. deep_data_prepare最后得到的数据如下：

   ```python
   train_x char shape is:  (102277, 2000)
   test_x char shape is:  (102277, 2000)
   train_x word shape is:  (102277, 2000)
   test_x word shape is:  (102277, 2000)
   train_y shape is:  (102277, 19)
   ```





##### init_embedding()

```python
def init_embedding(config, word2vec_model):
    vocab_len = len(word2vec_model.stoi) + 2
    print('Vocabulaty size : ', vocab_len)
    print('create embedding matrix')
    all_embs = np.stack(word2vec_model.embedding.values())
    embed_matrix = np.random.normal(all_embs.mean(), all_embs.std(), size=(vocab_len, config.EMBED_SIZE))
    embed_matrix[-2] = 0  # padding

    for i, val in tqdm(word2vec_model.embedding.items()):
        embed_matrix[i] = val
    return embed_matrix
```

对于word_seg_word2vec_model，Vocabulaty size = 359280
通过np.stack得到all_embs

```python
all_embs[1]
array([[ 1.1159024 , -0.9439267 ,  1.6538107 ,  0.51880634, -0.28107253,
         0.8258289 ,  0.4892525 , -0.86638635,  0.37004843,  0.14234634,
        -0.45239335, -0.6944821 , -0.4369281 , -0.8420563 ,  0.28929248,
         0.6599713 ,  1.3853775 , -0.2667373 , -0.7879151 , -0.64850277,
        -2.1579883 ,  0.501599  , -1.569721  ,  1.21212   , -0.28420335,
        -0.04119739,  2.73635   ,  1.0441073 , -0.3677843 , -0.05077151,
         0.34245312,  0.6541219 , -2.290163  , -0.7937558 , -0.04138851,
        -0.47539467, -1.6735233 ,  0.7398221 , -1.9585176 , -0.25061944,
         0.49877718, -1.1971399 , -1.8081696 ,  0.43393373, -0.9325331 ,
        -1.0779508 ,  0.318737  , -1.0975145 , -0.67270255,  0.76061845,
        -0.3231523 , -0.5454442 ,  1.5193787 , -0.31307316,  0.59510237,
         0.6517821 , -1.1277986 , -0.83355725,  0.8567625 , -0.19216359,
         0.64824337,  0.0971756 ,  0.00415489, -0.99654764, -0.61841565,
         1.9420384 ,  1.2276601 , -1.4297014 ,  2.1078956 ,  0.887553  ,
         0.0542723 , -0.30214202,  0.3926018 , -0.44055524,  0.54428256,
         0.99252826,  0.06819235,  0.2644587 , -0.5735893 ,  2.312784  ,
         0.5598059 , -0.9248841 ,  1.0703549 , -1.1437482 ,  0.9092705 ,
        -0.7779561 ,  0.449554  , -0.22552817,  1.4818811 , -1.2523264 ,
         0.17260194, -0.3289826 , -0.60421383,  0.5906034 , -0.7277698 ,
        -0.906046  , -0.79544777,  0.2908988 , -1.4678165 , -0.22682567]],
      dtype=float32)
```

all_embs.shape = (359278, 100)

embed_matrix.shape = (359280, 100) ，因为EMBED_SIZE设置为100

而embed_matrix其实算是all_embs 做了一个 size = 2 的padding。



### convlstm_model.py

从代码里可以看到，之所以train_predict.py里model.train_predict()可以运行，是因为model为convlstm，而ConvlstmModel调用了BasicDeepModel，从而BasicDeepModel类所有的内容都被调用。

BasicDeepModel负责保存所有的训练过程，而ConvlstmModel模型部分只有个[create_model](#create_model())模块，用来保存ConvlstmModel特异的结构部分。

```python
from keras.models import *
from keras.layers import *
from model.model_basic import BasicDeepModel
from model.model_component import AttentionWeightedAverage
from keras.utils.vis_utils import plot_model
from keras import regularizers


class ConvlstmModel(BasicDeepModel):
    def __init__(self, name='basicModel', num_flods=5, config=None):
        name = 'convlstm' + config.main_feature
        BasicDeepModel.__init__(self, name=name, n_folds=num_flods, config=config)

    def create_model(self):
        dropout_p = 0.2
        char_embedding = Embedding(self.max_c_features, self.char_embed_size, weights=[self.char_embedding], trainable=True, name='char_embedding')
        word_embedding = Embedding(self.max_w_features, self.word_embed_size, weights=[self.word_embedding], trainable=True, name='word_embedding')

        char_input = Input(shape=(self.char_max_len,), name='char')
        word_input = Input(shape=(self.word_max_len,), name='word')
        c_mask = Masking(mask_value=self.char_mask_value)(char_input)
        w_mask = Masking(mask_value=self.word_mask_value)(word_input)

        char_embedding = Embedding(self.max_c_features, self.char_embed_size, weights=[self.char_embedding], trainable=True, name='char_embedding')
        word_embedding = Embedding(self.max_w_features, self.word_embed_size, weights=[self.word_embedding], trainable=True, name='word_embedding')

        if self.config.main_feature == 'char':
            embedding = char_embedding
            input = char_input
            x = c_mask
        else:
            embedding = word_embedding
            input = word_input
            x = w_mask

        x = embedding(x)

        x = BatchNormalization()(x)
        x = SpatialDropout1D(dropout_p)(x)
        x = Conv1D(256, kernel_size=3, padding="valid", kernel_initializer="glorot_uniform")(x)
        x = Bidirectional(CuDNNLSTM(60, return_sequences=True))(x)
        x = SpatialDropout1D(dropout_p)(x)
        x = Bidirectional(CuDNNGRU(60, return_sequences=True))(x)

        avg_pool = GlobalAveragePooling1D()(x)
        max_pool = GlobalMaxPooling1D()(x)
        attn = AttentionWeightedAverage()(x)

        if self.config.main_feature == 'all':
            recurrent_units = 60
            word_embedding = Embedding(self.max_w_features, self.word_embed_size, weights=[self.word_embedding], trainable=False, name='word_embedding')
            word_input = Input(shape=(self.word_max_len,), name='word')
            word_embedding_layer = word_embedding(word_input)
            word_embedding_layer = SpatialDropout1D(0.2)(word_embedding_layer)

            word_rnn_1 = Bidirectional(CuDNNGRU(recurrent_units // 2, return_sequences=True))(word_embedding_layer)
            word_rnn_1 = SpatialDropout1D(0.1)(word_rnn_1)
            word_rnn_2 = Bidirectional(CuDNNGRU(recurrent_units // 2, return_sequences=True))(word_rnn_1)

            word_maxpool = GlobalMaxPooling1D()(word_rnn_2)
            word_average = GlobalAveragePooling1D()(word_rnn_2)

            concat2 = concatenate([avg_pool, max_pool, word_maxpool, word_average], axis=-1)
            concat2 = Dropout(0.5)(concat2)
            dense2 = Dense(self.n_class, activation="softmax")(concat2)
            res_model = Model(inputs=[char_input, word_input], outputs=dense2)
        else:
            concat2 = concatenate([attn, avg_pool, max_pool], axis=-1)
            concat2 = Dropout(dropout_p)(concat2)
            dense2 = Dense(self.n_class, activation="softmax", kernel_regularizer=regularizers.l2(self.wd))(concat2)
            res_model = Model(inputs=[input], outputs=dense2)

        plot_model(res_model, to_file="{}.png".format(self.name), show_shapes=True)
        return res_model


```


{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from config import GROUP_DICT, PAGE, SQL_DICT, HEADERS, PROXY_POOL_URL, MAX_GET_RETRY, OUTPUT_PATH, SPIDER_INTERVAL\n",
    "from base import _Sql_Base\n",
    "import requests\n",
    "import emoji\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "import os\n",
    "import argparse\n",
    "import re\n",
    "\n",
    "\n",
    "class HTTPError(Exception):\n",
    "\n",
    "    \"\"\" HTTP状态码不是200异常 \"\"\"\n",
    "\n",
    "    def __init__(self, status_code, url):\n",
    "        self.status_code = status_code\n",
    "        self.url = url\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"%s HTTP %s\" % (self.url, self.status_code)\n",
    "\n",
    "def get_logger(name):\n",
    "    \"\"\"logger\n",
    "    \"\"\"\n",
    "    default_logger = logging.getLogger(name)\n",
    "    default_logger.setLevel(logging.DEBUG)\n",
    "    stream = logging.StreamHandler()\n",
    "    stream.setLevel(logging.DEBUG)\n",
    "    formatter = logging.Formatter(\"[%(levelname)s] %(asctime)s - %(message)s\")\n",
    "    stream.setFormatter(formatter)\n",
    "    default_logger.addHandler(stream)\n",
    "    return default_logger    \n",
    "\n",
    "class Douban_corpus_spider(_Sql_Base):\n",
    "\n",
    "    def __init__(self, is_proxy = False):\n",
    "\n",
    "        self.GROUP_DICT = GROUP_DICT\n",
    "        self.PAGE = PAGE\n",
    "        self.sql_engine = self.create_engine(SQL_DICT)\n",
    "        self.is_proxy = is_proxy\n",
    "        if is_proxy:\n",
    "            self.proxyIP = self.get_proxy()\n",
    "        self.logger = get_logger(\"douban_spider\")\n",
    "            \n",
    "    def request_douban(self, url):\n",
    "        headers = {\n",
    "            'User-Agent': HEADERS['GalaxyS5']\n",
    "        }\n",
    "        for i in range(MAX_GET_RETRY):\n",
    "            try:\n",
    "                if self.is_proxy:\n",
    "                    proxyIP = self.proxyIP\n",
    "                    proxies = {\n",
    "                        'http' : proxyIP,\n",
    "                        'https': proxyIP\n",
    "                    }\n",
    "                    response = requests.get(url, proxies=proxies, headers=headers)\n",
    "                else:\n",
    "                    response = requests.get(url, headers=headers)\n",
    "                if response.status_code != 200:\n",
    "                    raise HTTPError(response.status_code, url)\n",
    "                print('successfully get data from %s' %url)\n",
    "                break\n",
    "            except Exception as exc:\n",
    "                self.logger.warn(\"%s %d failed!\\n%s\", url, i, str(exc))\n",
    "                if self.is_proxy:\n",
    "                    self.proxyIP = self.get_proxy()\n",
    "                continue\n",
    "#         time.sleep(2)\n",
    "        return response.text\n",
    "    \n",
    "    # 从代理池中随机取出一个IP\n",
    "    def get_proxy(self):\n",
    "        try:\n",
    "            response = requests.get(PROXY_POOL_URL)\n",
    "            if response.status_code == 200:\n",
    "                print('proxy: %s' %response.text)\n",
    "#                 time.sleep(1)\n",
    "                return re.sub( '[\\\\r\\\\n]+', '',\"http://%s\" %response.text)\n",
    "        except ConnectionError:\n",
    "            return None\n",
    "\n",
    "    def spider_links(self, group, page):\n",
    "        url = '{}discussion?start={}'.format(self.GROUP_DICT[group], str(page*25))\n",
    "        html = self.request_douban(url)\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        list_ = soup.find(class_='olt').find_all('tr')\n",
    "        page_link = []; page_title = []\n",
    "        for item in list_:\n",
    "            try:\n",
    "                page_link.append(item.find('a').get('href'))\n",
    "                page_title.append(emoji.demojize(item.find('a').get('title')))\n",
    "            except:\n",
    "                continue\n",
    "        return page_link, page_title\n",
    "\n",
    "    def spider_page(self, url):\n",
    "        html = self.request_douban(url)\n",
    "        soup = BeautifulSoup(html, 'lxml')\n",
    "        page_author_diag = []\n",
    "        for item in soup.find(class_=\"note-content paper\").find_all('p'):\n",
    "            if len(item) > 0:\n",
    "                page_author_diag.append(emoji.demojize(item.contents[0]))\n",
    "        list_ = soup.find_all(class_ = 'content')\n",
    "        page_comments = []\n",
    "        for item in list_:\n",
    "            page_comments.append(emoji.demojize(item.contents[2].replace(' ','').replace('\\n','')))\n",
    "        return page_author_diag, page_comments\n",
    "\n",
    "    def spider_group(self, group, page):\n",
    "        min_page = page[0]\n",
    "        max_page = page[1]\n",
    "        spider_outputs = {}\n",
    "        link_list = []\n",
    "        title_list = []\n",
    "        for page in range(min_page, max_page):\n",
    "            output_page = {}\n",
    "            link_list_page, title_list_page = self.spider_links(group, page)\n",
    "            for link in link_list_page:\n",
    "                try:\n",
    "                    spider_outputs[link] = {}\n",
    "                    spider_outputs[link]['title'] = title_list_page[link_list_page.index(link)]\n",
    "                    spider_outputs[link]['author_diag'], spider_outputs[link]['comments'] = self.spider_page(link)\n",
    "                    output_page[link] = spider_outputs[link]\n",
    "                except:\n",
    "                    print('%s fail' %link)\n",
    "                    continue\n",
    "            self.json_write(output_page, os.path.join(OUTPUT_PATH, '{}_{}.json'.format(group,page)))      \n",
    "        return spider_outputs\n",
    "\n",
    "    def group_dict_transfer(self, output_dict):\n",
    "        data = pd.DataFrame(output_dict).T\n",
    "        data['link'] = data.index\n",
    "        data = data.reset_index(drop = True)[['link','title','author_diag','comments']]\n",
    "        data['author_diag'] = data['author_diag'].apply(lambda x: x[0] if x else '')\n",
    "        data['comments'] = data['comments'].apply(lambda x: '|'.join(x))\n",
    "        return data\n",
    "\n",
    "    def run(self):\n",
    "        for group in self.GROUP_DICT.keys():\n",
    "            output_dict = self.spider_group(group, self.PAGE)\n",
    "            output_table = self.group_dict_transfer(output_dict)\n",
    "            self.table_save(output_table, group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proxy: 58.218.200.248:6289\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dcs = Douban_corpus_spider(is_proxy = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2400/25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully get data from https://www.douban.com/group/kuakua/discussion?start=1125\n",
      "successfully get data from https://www.douban.com/group/topic/141537628/\n",
      "successfully get data from https://www.douban.com/group/topic/141369017/\n",
      "successfully get data from https://www.douban.com/group/topic/141523711/\n",
      "successfully get data from https://www.douban.com/group/topic/141536855/\n",
      "successfully get data from https://www.douban.com/group/topic/141536767/\n",
      "successfully get data from https://www.douban.com/group/topic/141479303/\n",
      "successfully get data from https://www.douban.com/group/topic/141242035/\n",
      "successfully get data from https://www.douban.com/group/topic/141442729/\n",
      "successfully get data from https://www.douban.com/group/topic/141531957/\n",
      "successfully get data from https://www.douban.com/group/topic/141211474/\n",
      "successfully get data from https://www.douban.com/group/topic/141494121/\n",
      "successfully get data from https://www.douban.com/group/topic/141530896/\n",
      "successfully get data from https://www.douban.com/group/topic/141488819/\n",
      "successfully get data from https://www.douban.com/group/topic/141531087/\n",
      "successfully get data from https://www.douban.com/group/topic/141527694/\n",
      "successfully get data from https://www.douban.com/group/topic/141469734/\n",
      "successfully get data from https://www.douban.com/group/topic/141444908/\n",
      "successfully get data from https://www.douban.com/group/topic/141525607/\n",
      "successfully get data from https://www.douban.com/group/topic/141185141/\n",
      "successfully get data from https://www.douban.com/group/topic/141516663/\n",
      "successfully get data from https://www.douban.com/group/topic/141519068/\n",
      "successfully get data from https://www.douban.com/group/topic/141441411/\n",
      "successfully get data from https://www.douban.com/group/topic/141523805/\n",
      "successfully get data from https://www.douban.com/group/topic/141523492/\n",
      "successfully get data from https://www.douban.com/group/topic/141512084/\n"
     ]
    }
   ],
   "source": [
    "dcs.PAGE = [45,46]\n",
    "\n",
    "dcs.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
